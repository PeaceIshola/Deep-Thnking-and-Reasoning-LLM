{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe0fb30d",
   "metadata": {},
   "source": [
    "# Project 4: **Build a Deep Research System**\n",
    "Welcome to project 4! For this project, we shift our focus from tool use and agents to *reasoning* models. You will practice state‑of‑the‑art inference‑time scaling methods such as *Chain‑of‑Thought* prompting and *Tree‑of‑Thoughts*, and briefly explore high-levels of training reasoning models using techniques like **STaR**.\n",
    "\n",
    "\n",
    "Finally, you will put everything together to build a *deep research agent* that can browse the web, reason over what it finds, and give structured answers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54845369",
   "metadata": {},
   "source": [
    "## Learning Objectives  \n",
    "* Apply common inference‑time scaling methods: **zero‑shot / few‑shot CoT, self‑consistency, sequential decoding, tree‑of‑thoughts**  \n",
    "* Gain intuition for **training** reasoning‑capable models following **STaR** approach \n",
    "* Build a minimal **deep‑research agent** that combines step‑by‑step reasoning with live web search   \n",
    "* Practice extending deep-search to a multi-agent system "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a40a86",
   "metadata": {},
   "source": [
    "## Roadmap  \n",
    "1. Environment setup  \n",
    "2. Inference‑time scaling  \n",
    "   2.1 Few‑shot & zero‑shot CoT  \n",
    "   2.2 Self‑consistency\n",
    "   2.3 Sequential revisions  \n",
    "   2.4 Tree‑of‑Thought\n",
    "3. STaR for training models for reasoning  \n",
    "4. Deep-research agent  \n",
    "5. (Optional) Multi-agent deep-research"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e480f76",
   "metadata": {},
   "source": [
    "# 1‑ Environment setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17c2218",
   "metadata": {},
   "source": [
    "## 1.1- Conda environment\n",
    "\n",
    "Before we start coding, you need a reproducible setup. Open a terminal in the same directory as this notebook and run:\n",
    "\n",
    "```bash\n",
    "# Create and activate the conda environment\n",
    "conda env create -f environment.yaml && conda activate deep_research\n",
    "\n",
    "# Register this environment as a Jupyter kernel\n",
    "python -m ipykernel install --user --name=deep_research --display-name \"deep_research\"\n",
    "```\n",
    "Once this is done, you can select \"deep_research\" from the Kernel → Change Kernel menu in Jupyter or VS Code.\n",
    "\n",
    "## 1.2 Ollama setup\n",
    "\n",
    "In this project we use the `llama3.2:3b` and `deepseek-r1:8b` models. You can try other smaller or larger reasoning LLMs such as `qwen2.5:3b-instruct` or `phi4-mini` to compare performance. Explore available models here: https://ollama.com/library.\n",
    "\n",
    "```bash\n",
    "ollama pull llama3.2:3b\n",
    "ollama pull deepseek-r1:8b\n",
    "# Additional small reasoning models to compare\n",
    "# ollama pull qwen2.5:3b-instruct\n",
    "# ollama pull phi4-mini\n",
    "\n",
    "```\n",
    "\n",
    "`ollama pull` downloads the model so you can run it locally without API calls."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e8d1b7",
   "metadata": {},
   "source": [
    "---  \n",
    "# 2‑ Inference‑time scaling\n",
    "\n",
    "Inference-time scaling refers to techniques that make an existing model reason better without retraining it. Instead of changing the model’s weights, we achieve reasoning capability by adjusting how we prompt, sample, or aggregate LLM's outputs.\n",
    "\n",
    "In this section, we’ll explore several inference-time strategies that improve reasoning quality using a non-reasoning base model. You will experiment with and compare methods such as:\n",
    "\n",
    "- Few-shot Chain-of-Thought (CoT)\n",
    "- Zero-shot CoT\n",
    "- Self-consistency\n",
    "- Sequential revision\n",
    "- Tree-of-Thoughts (ToT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081d499a",
   "metadata": {},
   "source": [
    "### 2.1: Few‑Shot CoT\n",
    "Few-shot prompting helps a model reason by showing one or multiple examples before asking a new question. By observing the pattern of reasoning and final answers, the model learns how to structure its own reasoning process on the new input.\n",
    "\n",
    "In this exercise, you will create a prompt that includes a few example Q&A pairs demonstrating step-by-step reasoning. Then, you will feed a new question and see the model’s output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173d73f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Write a few examples showing reasoning steps\n",
    "# Step 2: Write your new question\n",
    "# Step 3: Concatenate examples + new question into a single prompt\n",
    "# Step 4: Call your Ollama or OpenAI client to get a response from llama3.2:3b # e.g., client.chat.completions.create(...)\n",
    "# Step 5: Print the final answer\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "# Initialize Ollama client (uses OpenAI-compatible API)\n",
    "client = OpenAI(api_key=\"ollama\", base_url=\"http://localhost:11434/v1\")\n",
    "\n",
    "# Step 1 & 2: Create few-shot examples with reasoning steps\n",
    "few_shot_examples = \"\"\"\n",
    "Q: If a train travels 60 miles in 1 hour, how far will it travel in 3.5 hours at the same speed?\n",
    "A: Let me think step by step:\n",
    "1. The train's speed is 60 miles per hour\n",
    "2. To find distance, I multiply speed × time\n",
    "3. Distance = 60 miles/hour × 3.5 hours = 210 miles\n",
    "Therefore, the train will travel 210 miles.\n",
    "\n",
    "Q: A store has 48 apples. If they sell 1/3 of them in the morning and 1/4 of the remaining in the afternoon, how many apples are left?\n",
    "A: Let me think step by step:\n",
    "1. Morning sales: 1/3 of 48 = 48 ÷ 3 = 16 apples sold\n",
    "2. Remaining after morning: 48 - 16 = 32 apples\n",
    "3. Afternoon sales: 1/4 of 32 = 32 ÷ 4 = 8 apples sold\n",
    "4. Final remaining: 32 - 8 = 24 apples\n",
    "Therefore, 24 apples are left.\n",
    "\n",
    "Q: If 5 machines can produce 100 widgets in 2 hours, how many widgets can 8 machines produce in 3 hours?\n",
    "A: Let me think step by step:\n",
    "\"\"\"\n",
    "\n",
    "# Step 3: Call the model to complete the reasoning\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama3.2:3b\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that solves problems step by step.\"},\n",
    "        {\"role\": \"user\", \"content\": few_shot_examples}\n",
    "    ],\n",
    "    temperature=0.7,\n",
    "    max_tokens=300\n",
    ")\n",
    "\n",
    "# Step 5: Print the final answer\n",
    "print(\"=== Few-Shot Chain-of-Thought Response ===\\n\")\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698e29d9",
   "metadata": {},
   "source": [
    "### (Optional) Few-shot CoT on GPT2\n",
    "GPT-2 is a pre-trained language model without instruction tuning. It continues text rather than answering questions. In this section, you'll try the exact same CoT pattern on GPT-2 and observe what happens. The goal is to test whether few-shot CoT alone can elicit structured reasoning from a non-chat LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8af711f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "# Step 1: Load GPT-2 text-generation from huggingface (https://huggingface.co/docs/transformers/en/model_doc/gpt2)\n",
    "# Step 2: Write 1–2 few-shot reasoning examples (short, explicit steps + final answer in your own unique format)\n",
    "# Step 3: Append a new test question after the examples to form one prompt string\n",
    "# Step 4: Generate 1–3 completions with different decoding settings (e.g., greedy vs. top-k)\n",
    "# Step 5: Print raw outputs; check if steps are followed and if the final answer is correct\n",
    "\n",
    "# Step 1: Load GPT-2 text generation pipeline\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "\n",
    "# Step 2 & 3: Create few-shot examples and append new question\n",
    "prompt = \"\"\"Q: If I have 3 apples and buy 2 more, how many do I have?\n",
    "A: Step 1: Start with 3 apples. Step 2: Add 2 more apples. Step 3: 3 + 2 = 5. Answer: 5 apples.\n",
    "\n",
    "Q: A rectangle has length 6 and width 4. What is its area?\n",
    "A: Step 1: Area formula is length × width. Step 2: 6 × 4 = 24. Answer: 24.\n",
    "\n",
    "Q: If a book costs $12 and I have $50, how much change will I get?\n",
    "A:\"\"\"\n",
    "\n",
    "# Step 4: Generate completions with different decoding strategies\n",
    "print(\"=== GPT-2 Few-Shot CoT Results ===\\n\")\n",
    "\n",
    "# Greedy decoding (temperature=0, deterministic)\n",
    "print(\"--- Greedy Decoding ---\")\n",
    "output1 = generator(prompt, max_length=len(prompt.split()) + 50, do_sample=False, pad_token_id=50256)\n",
    "print(output1[0]['generated_text'][len(prompt):])\n",
    "\n",
    "# Top-k sampling (more diverse)\n",
    "print(\"\\n--- Top-k Sampling (k=50) ---\")\n",
    "output2 = generator(prompt, max_length=len(prompt.split()) + 50, do_sample=True, top_k=50, temperature=0.8, pad_token_id=50256)\n",
    "print(output2[0]['generated_text'][len(prompt):])\n",
    "\n",
    "# Nucleus sampling (top-p)\n",
    "print(\"\\n--- Nucleus Sampling (p=0.9) ---\")\n",
    "output3 = generator(prompt, max_length=len(prompt.split()) + 50, do_sample=True, top_p=0.9, temperature=0.7, pad_token_id=50256)\n",
    "print(output3[0]['generated_text'][len(prompt):])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adee0e7",
   "metadata": {},
   "source": [
    "### 2.2: Zero‑Shot Chain‑of‑Thought\n",
    "Zero-shot CoT encourages the model to reason without examples by adding a short cue such as “Let’s think step by step.” This simple phrase often activates the model’s latent reasoning ability even when no demonstrations are provided. It serves as a baseline to compare with few-shot and other inference-time scaling methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c444eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Step 1: Write the question and a zero-shot CoT cue (e.g., \"Let's think step by step.\")\n",
    "# Step 2: Build a single prompt string that includes brief role guidance plus the question\n",
    "# Step 3: Call your Ollama or OpenAI client to get a response from llama3.2:3b  # e.g., client.chat.completions.create(...)\n",
    "# Step 4: Print the chain and the final answer\n",
    "\n",
    "# Initialize Ollama client\n",
    "client = OpenAI(api_key=\"ollama\", base_url=\"http://localhost:11434/v1\")\n",
    "\n",
    "# Step 1: Create question with zero-shot CoT cue\n",
    "question = \"\"\"\n",
    "If a car travels at 45 mph for the first 2 hours, then increases speed to 60 mph for the next 3 hours, what is the total distance traveled?\n",
    "\n",
    "Let's think step by step.\n",
    "\"\"\"\n",
    "\n",
    "# Step 2 & 3: Build prompt and call the model\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama3.2:3b\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that solves problems with clear step-by-step reasoning.\"},\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ],\n",
    "    temperature=0.7,\n",
    "    max_tokens=300\n",
    ")\n",
    "\n",
    "# Step 4: Print the reasoning chain and final answer\n",
    "print(\"=== Zero-Shot Chain-of-Thought Response ===\\n\")\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686708da",
   "metadata": {},
   "source": [
    "### 2.3 Self‑Consistency\n",
    "Self-consistency enhances reasoning accuracy by sampling multiple independent reasoning paths for the same question instead of relying on a single deterministic answer. Each run may follow a slightly different logical chain, and the diversity helps correct individual mistakes. After generating several reasoning traces, you then aggregate the final answers using majority voting.\n",
    "\n",
    "This approach is especially useful when tasks involve multi-step reasoning or arithmetic, where single-path outputs may be incorrect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2fb325",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import re, collections\n",
    "\n",
    "client = OpenAI(api_key = \"ollama\", base_url = \"http://localhost:11434/v1\")\n",
    "MODEL = \"llama3.2:3b\"\n",
    "\n",
    "def cot_answer(question, temperature=1.0):\n",
    "    # Generate a step-by-step reasoning chain for the given question and extract the final answer.\n",
    "    prompt = f\"{question}\\n\\nLet's think step by step.\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that solves problems step by step.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=temperature,\n",
    "        max_tokens=200\n",
    "    )\n",
    "    \n",
    "    text = response.choices[0].message.content\n",
    "    # Extract the final answer (look for common patterns like \"Answer:\", \"Therefore\", or numbers)\n",
    "    match = re.search(r'(?:answer|result|therefore)[:\\s]+([^\\n\\.]+)|(\\d+(?:\\.\\d+)?)\\s*(?:$|\\.)', text.lower())\n",
    "    if match:\n",
    "        return match.group(1) or match.group(2)\n",
    "    return text.split()[-1]  # fallback: return last word\n",
    "\n",
    "def self_consistent(question, n=10):\n",
    "    # Run multiple reasoning chains and select the most frequent final answer by majority voting.\n",
    "    answers = []\n",
    "    \n",
    "    print(f\"Running {n} independent reasoning chains...\\n\")\n",
    "    for i in range(n):\n",
    "        # Use different temperatures for diversity\n",
    "        temp = 0.7 + (i % 3) * 0.15  # Varies between 0.7, 0.85, 1.0\n",
    "        answer = cot_answer(question, temperature=temp)\n",
    "        # Normalize answer (strip, lowercase for better matching)\n",
    "        normalized = answer.strip().lower()\n",
    "        answers.append(normalized)\n",
    "        print(f\"Run {i+1}: {normalized}\")\n",
    "    \n",
    "    # Count occurrences and return most common\n",
    "    counter = collections.Counter(answers)\n",
    "    winner = counter.most_common(1)[0][0]\n",
    "    \n",
    "    return winner, counter\n",
    "\n",
    "\n",
    "question = \"What is the square root of 144?\"\n",
    "winner, counter = self_consistent(question)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Votes:\", counter)\n",
    "print(\"Chosen answer:\", winner)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bea715",
   "metadata": {},
   "source": [
    "### 2.4: Sequential Revision\n",
    "\n",
    "Sequential revision iteratively improves an answer by generating a first draft, critiquing it, and producing revised drafts that condition on prior answers. Each round should be short and focused, so improvements accumulate without drifting from the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e5859d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=\"ollama\", base_url=\"http://localhost:11434/v1\")\n",
    "MODEL = \"llama3.2:3b\"\n",
    "\n",
    "def sequential_revision(question: str, max_steps: int = 3) -> str:\n",
    "    # Generate an initial draft answer, then iteratively refine it by conditioning each revision on the previous one.\n",
    "    # Step 1: Ask the model to produce the first draft for the given question\n",
    "    # Step 2: Loop for max_steps-1 times, each time feeding the last draft back to the model with a request to revise\n",
    "    # Step 3: Print each draft to observe how the answer evolves\n",
    "    # Step 4: Return the final improved draft\n",
    "    \n",
    "    # Step 1: Generate initial draft\n",
    "    print(\"=\"*60)\n",
    "    print(\"INITIAL DRAFT\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that provides clear, comprehensive answers.\"},\n",
    "            {\"role\": \"user\", \"content\": question}\n",
    "        ],\n",
    "        temperature=0.7,\n",
    "        max_tokens=300\n",
    "    )\n",
    "    \n",
    "    draft = response.choices[0].message.content\n",
    "    print(draft)\n",
    "    print()\n",
    "    \n",
    "    # Step 2 & 3: Iteratively revise the draft\n",
    "    for step in range(1, max_steps):\n",
    "        print(\"=\"*60)\n",
    "        print(f\"REVISION {step}\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        revision_prompt = f\"\"\"Here is a previous answer to the question: \"{question}\"\n",
    "\n",
    "Previous answer:\n",
    "{draft}\n",
    "\n",
    "Please review and improve this answer by:\n",
    "1. Adding more detail or clarity where needed\n",
    "2. Correcting any errors or imprecisions\n",
    "3. Making it more concise and well-structured\n",
    "\n",
    "Provide an improved version:\"\"\"\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=MODEL,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant that carefully revises and improves answers.\"},\n",
    "                {\"role\": \"user\", \"content\": revision_prompt}\n",
    "            ],\n",
    "            temperature=0.7,\n",
    "            max_tokens=300\n",
    "        )\n",
    "        \n",
    "        draft = response.choices[0].message.content\n",
    "        print(draft)\n",
    "        print()\n",
    "    \n",
    "    # Step 4: Return final draft\n",
    "    return draft\n",
    "\n",
    "\n",
    "# Step 1: Define a question that benefits from multi-step reasoning\n",
    "question = \"Explain how photosynthesis works and why it's important for life on Earth.\"\n",
    "\n",
    "# Step 2: Call sequential_revision\n",
    "print(\"Question:\", question)\n",
    "print()\n",
    "final_answer = sequential_revision(question, max_steps=3)\n",
    "\n",
    "# Step 3: Print the final output\n",
    "print(\"=\"*60)\n",
    "print(\"FINAL ANSWER\")\n",
    "print(\"=\"*60)\n",
    "print(final_answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9319ee8",
   "metadata": {},
   "source": [
    "### 2.5 Tree‑of‑Thoughts\n",
    "Tree-of-Thoughts reframes reasoning as a search process rather than a single forward chain.\n",
    "Instead of producing one linear sequence of thoughts, the model generates multiple candidate thoughts at each step, evaluates their promise, and then expands only the best few. This allows exploration of different reasoning paths before committing to a final answer, similar to how humans brainstorm, prune, and refine ideas.\n",
    "\n",
    "\n",
    "In this section, you’ll experiment with two simplified versions of ToT:\n",
    "1. Word Ladder puzzle solver: a small example where each “thought” is a candidate word transition.\n",
    "2. Generic ToT search (depth 2, width 2): a minimal logic to expand, evaluate, and select reasoning branches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d047801",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Word Ladder Puzzle ##########\n",
    "\n",
    "def neighbors(word, vocabulary):\n",
    "    # Generate all valid one-letter mutations of 'word' that exist in 'vocabulary' and return them.\n",
    "    results = []\n",
    "    for i in range(len(word)):\n",
    "        for c in 'abcdefghijklmnopqrstuvwxyz':\n",
    "            if c != word[i]:\n",
    "                # Create a new word by replacing character at position i\n",
    "                candidate = word[:i] + c + word[i+1:]\n",
    "                if candidate in vocabulary:\n",
    "                    results.append(candidate)\n",
    "    return results\n",
    "\n",
    "\n",
    "def edit_distance(word1, word2):\n",
    "    # Calculate Levenshtein distance between two words\n",
    "    if len(word1) != len(word2):\n",
    "        return sum(1 for a, b in zip(word1, word2) if a != b) + abs(len(word1) - len(word2))\n",
    "    return sum(1 for a, b in zip(word1, word2) if a != b)\n",
    "\n",
    "\n",
    "def tree_of_thought(start, goal, vocab, max_depth=5, beam_width=4):\n",
    "    # Search over partial thoughts (paths) using a small beam.\n",
    "    # Step 1: Initialize the frontier with a single path [start]\n",
    "    # Step 2: For each depth, expand each path by one neighbor from 'neighbors'\n",
    "    # Step 3: Score paths by edit distance between last word and 'goal' (smaller is better)\n",
    "    # Step 4: Keep the top 'beam_width' paths and stop early if any reaches 'goal'\n",
    "    # Step 5: Return the best goal-reaching path or None\n",
    "    \n",
    "    # Step 1: Initialize frontier\n",
    "    frontier = [[start]]\n",
    "    visited = {start}\n",
    "    \n",
    "    # Step 2: Beam search for max_depth iterations\n",
    "    for depth in range(max_depth):\n",
    "        candidates = []\n",
    "        \n",
    "        # Expand each path in the frontier\n",
    "        for path in frontier:\n",
    "            last_word = path[-1]\n",
    "            \n",
    "            # Step 4: Check if we reached the goal\n",
    "            if last_word == goal:\n",
    "                return path\n",
    "            \n",
    "            # Get all neighbors\n",
    "            for neighbor in neighbors(last_word, vocab):\n",
    "                if neighbor not in visited:\n",
    "                    new_path = path + [neighbor]\n",
    "                    # Step 3: Score by edit distance (lower is better)\n",
    "                    score = edit_distance(neighbor, goal)\n",
    "                    candidates.append((score, new_path))\n",
    "                    visited.add(neighbor)\n",
    "        \n",
    "        if not candidates:\n",
    "            break\n",
    "        \n",
    "        # Step 4: Keep top beam_width paths (sorted by score, ascending)\n",
    "        candidates.sort(key=lambda x: x[0])\n",
    "        frontier = [path for score, path in candidates[:beam_width]]\n",
    "    \n",
    "    # Step 5: Return best path if goal was reached, otherwise None\n",
    "    for path in frontier:\n",
    "        if path[-1] == goal:\n",
    "            return path\n",
    "    return None\n",
    "\n",
    "\n",
    "vocab = {\"hit\",\"dot\",\"cog\",\"log\",\"dog\",\"lot\",\"lit\",\"hot\"}\n",
    "result = tree_of_thought(\"hit\", \"cog\", vocab)\n",
    "print(\"Solution path:\", result)  # one candidate solution: ['hit', 'hot', 'dot', 'dog', 'cog']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89067302",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Generic ToT Search ##########\n",
    "\n",
    "import re\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=\"ollama\", base_url=\"http://localhost:11434/v1\")\n",
    "MODEL = \"llama3.2:3b\"\n",
    "\n",
    "def propose_thoughts(question, state, k=2):\n",
    "    # Propose up to k next \"thoughts\" that extend the current partial solution/state.\n",
    "    # Steps: build a short prompt with problem + current state; call your client with n=k. Then return a list of stripped strings (≤ k).\n",
    "    \n",
    "    if state:\n",
    "        prompt = f\"Problem: {question}\\n\\nCurrent partial solution:\\n{state}\\n\\nPropose the next step to continue this solution. Be brief and specific:\"\n",
    "    else:\n",
    "        prompt = f\"Problem: {question}\\n\\nPropose the first step toward solving this. Be brief and specific:\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.8,\n",
    "        max_tokens=100,\n",
    "        n=k  # Generate k completions\n",
    "    )\n",
    "    \n",
    "    # Extract and return the thoughts\n",
    "    thoughts = [choice.message.content.strip() for choice in response.choices]\n",
    "    return thoughts[:k]  # Ensure we return at most k\n",
    "\n",
    "\n",
    "def score_state(question, state):\n",
    "    # Score how promising a partial solution is on a 1–10 scale (higher is better).\n",
    "    # Steps: build a rating prompt; call the model; parse the first integer 1–10;\n",
    "    \n",
    "    prompt = f\"\"\"Problem: {question}\n",
    "\n",
    "Partial solution:\n",
    "{state}\n",
    "\n",
    "Rate how promising this partial solution is on a scale of 1-10, where:\n",
    "- 1-3: Poor approach, unlikely to lead to a good solution\n",
    "- 4-6: Okay approach, has some potential\n",
    "- 7-9: Good approach, likely to lead to a solid solution\n",
    "- 10: Excellent approach, very promising\n",
    "\n",
    "Respond with just a number from 1 to 10:\"\"\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.3,\n",
    "        max_tokens=10\n",
    "    )\n",
    "    \n",
    "    text = response.choices[0].message.content.strip()\n",
    "    # Parse the first integer between 1-10\n",
    "    match = re.search(r'\\b([1-9]|10)\\b', text)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    return 5  # Default middle score if parsing fails\n",
    "\n",
    "\n",
    "def tree_of_thoughts(question, depth=2, width=2):\n",
    "    # Run a tiny ToT search: expand states with propose_thoughts, score with score_state, keep top-k at each depth.\n",
    "    # Steps: initialize frontier=[(\"\", 0)]; for each depth, expand each state with k=width thoughts; score each; sort by score desc; keep top 'width'; return best state and score.\n",
    "    \n",
    "    # Initialize frontier with empty state\n",
    "    frontier = [(\"\", 0)]\n",
    "    \n",
    "    print(f\"=== Tree-of-Thoughts Search (depth={depth}, width={width}) ===\\n\")\n",
    "    \n",
    "    for d in range(depth):\n",
    "        print(f\"--- Depth {d+1} ---\")\n",
    "        candidates = []\n",
    "        \n",
    "        # Expand each state in frontier\n",
    "        for state, prev_score in frontier:\n",
    "            # Generate k=width new thoughts\n",
    "            thoughts = propose_thoughts(question, state, k=width)\n",
    "            \n",
    "            for thought in thoughts:\n",
    "                # Build new state by appending thought\n",
    "                if state:\n",
    "                    new_state = f\"{state}\\n\\nStep {d+2}: {thought}\"\n",
    "                else:\n",
    "                    new_state = f\"Step 1: {thought}\"\n",
    "                \n",
    "                # Score the new state\n",
    "                score = score_state(question, new_state)\n",
    "                candidates.append((new_state, score))\n",
    "                print(f\"  Score {score}: {thought[:60]}...\")\n",
    "        \n",
    "        # Sort by score (descending) and keep top 'width'\n",
    "        candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "        frontier = candidates[:width]\n",
    "        print()\n",
    "    \n",
    "    # Return the best state and its score\n",
    "    best_state, best_score = frontier[0]\n",
    "    return best_state, best_score\n",
    "\n",
    "\n",
    "question = \"Design a plan for a weekend science workshop for 12-year-olds.\"\n",
    "solution, score = tree_of_thoughts(question, depth=2, width=2)\n",
    "\n",
    "print(f\"=== Best Solution (score {score}) ===\\n{solution}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72dc38f6",
   "metadata": {},
   "source": [
    "---  \n",
    "# 3‑ Training Models for Reasoning\n",
    "\n",
    "### 3.1: CoT Training\n",
    "Chain-of-Thought (CoT) training conditions the model on explicit rationales during fine-tuning. Instead of teaching the model to output only the final answer, we train on (question, rationale, answer) so the model learns to internalize multi-step reasoning patterns. A practical recipe is STaR (Self-Taught Reasoner), which uses a stronger teacher model to bootstrap rationales that a smaller student can learn from.\n",
    "\n",
    "For tasks that require multi-hop reasoning, models fine-tuned on rationales often achieve higher accuracy and are more stable at inference time than models trained on direct answers only. \n",
    "\n",
    "Training a full language model is beyond the scope of this notebook, but here is the high-level workflow followed by a short pseudocode:\n",
    "- Collect questions: Prepare a dataset of questions and correct answers.\n",
    "- Generate rationales: Use a strong LLM to produce step-by-step reasoning ending with the correct answer.\n",
    "- Filter and clean: Discard incorrect or low-quality rationales.\n",
    "- Prepare training data: Format triples (question, rationale, answer) for supervised fine-tuning.\n",
    "- Fine-tune: Fine-tune the LLM on rationales.\n",
    "- Iterate: Refine prompts, improve data quality, and retrain for stronger reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb7cfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pseudocode (STaR loop)\n",
    "# for round in 1 ... iters:\n",
    "    # STEP 1: self-generate reasoning (teacher creates rationale + answer)\n",
    "    # STEP 2: keep only correct, high-quality traces\n",
    "    # STEP 3: fine-tune student on (question, rationale, answer) data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b53c70",
   "metadata": {},
   "source": [
    "### 3.2: ORM vs PRM + RL\n",
    "Training a Reward Model (RM) allows large language models to be improved through reinforcement learning (RL). Instead of fine-tuning directly on examples, we train a separate model that can score or rank model outputs, and use those scores as feedback signals to refine the policy model.\n",
    "\n",
    "Two main reward modeling approaches are ORM (predicts a scalar reward for the final answer) and PRM (evaluates the reasoning steps instead of just the outcome)\n",
    "\n",
    "\n",
    "\n",
    "| Approach | Typical loss | When to use |\n",
    "|-----------|-------------|-------------|\n",
    "|*Outcome Reward Model* | Predict scalar reward | Easy to collect training data using verifiers |\n",
    "|*Process Reward Model* | Predict rewards per step | Difficult to collect training data but more accurate |\n",
    "| *RLHF* | Use RM as reward in **RL** fine‑tuning | Aligns policy with human signals | Aligns model policy with human or synthetic preferences\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595635aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for round = 1 ... iters:\n",
    "    # STEP 1:  Generate reasoning\n",
    "        # sample a minibatch of questions\n",
    "        # policy roll‑out (actions + log‑probs)\n",
    "    # STEP 2:  Score the trajectory\n",
    "        # ORM: scalar reward for the final answer / PRM: scalar reward for the thought process\n",
    "    # STEP 3:  Reinforce the policy (PPO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545a81a6",
   "metadata": {},
   "source": [
    "---  \n",
    "# 4‑ A Deep Research Agent\n",
    "\n",
    "A deep-research agent pairs a reasoning model (e.g., deepseek-r1) with external tools for web search and retrieval. We will follow the ReAct pattern: the model writes short thoughts, decides when to call tools, reads observations, and continues reasoning until it can answer or reaches a step limit.\n",
    "\n",
    "We now combine a **search tool** with a reasoning model (e.g., `deepseek-r1`) in a multi-step setup. We follow the *ReAct* pattern (reason → tool → observation):\n",
    "\n",
    "1. The model reasoins and decides to use tools\n",
    "2. The agent searches and feed condensed snippets back as context\n",
    "3. Iterate until the model answers or hits a step limit\n",
    "\n",
    "We use `AgentType.OPENAI_FUNCTIONS`, which hides the loop inside the LangChain agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1e1648",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ddgs import DDGS\n",
    "from langchain.tools import Tool\n",
    "\n",
    "def ddg_search(query: str, k: int = 5) -> str:\n",
    "    # Use DDGS to run a simple web search and return joined snippets.\n",
    "    ddgs = DDGS()\n",
    "    results = ddgs.text(query, max_results=k)\n",
    "    snippets = []\n",
    "    for r in results:\n",
    "        snippets.append(f\"{r['title']}: {r['body']}\")\n",
    "    return \"\\n\\n\".join(snippets)\n",
    "\n",
    "search_tool = Tool(\n",
    "    name=\"DuckDuckGo Search\",\n",
    "    func=ddg_search,\n",
    "    description=\"Search the public web. Input: a plain English query. Returns: concatenated snippets.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418a0d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent, AgentType\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "MODEL = \"deepseek-r1:8b\"\n",
    "question = \"What are the best resources to learn machine learning in 2025?\"\n",
    "\n",
    "# Step 1: Initialize the reasoning model via ChatOllama\n",
    "llm = ChatOllama(model=MODEL, temperature=0.7)\n",
    "\n",
    "# Step 2: Build the agent with tool access (DuckDuckGo Search) and function-calling interface (initialize_agent)\n",
    "agent = initialize_agent([search_tool], llm, agent=AgentType.OPENAI_FUNCTIONS, verbose=True)\n",
    "\n",
    "# Step 3: Ask a query and let the agent search + reason to produce an answer\n",
    "result = agent.run(question)\n",
    "print(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
